{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c63a67ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import colorcet as cc\n",
    "import seaborn as sns\n",
    "import rasterio as rio\n",
    "import geopandas as gp\n",
    "\n",
    "import fiona \n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.io.img_tiles as cimgt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from rasterio.mask import mask\n",
    "from cartopy.io.shapereader import Reader\n",
    "from cartopy.feature import ShapelyFeature\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "from matplotlib.lines import Line2D      \n",
    "from matplotlib.cm import get_cmap\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict \n",
    "\n",
    "from osgeo import gdal, osr, ogr\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40ade9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "\n",
    "def plot_basemaps(image, shp, title = None, cmap = 'viridis', ax = None, latlabels = True, lonlabels = True, vmax = None):\n",
    "    imextent = gp.read_file(shp).set_crs(\"EPSG:4326\").to_crs(4326).total_bounds\n",
    "    shape_feature = ShapelyFeature(Reader(\"../shape/sierra_catchments.shp\").geometries(),\n",
    "                        ccrs.PlateCarree(), edgecolor='white', facecolor = 'none')\n",
    "\n",
    "    points = list(Reader(\"../shape/trm_res_fnf.shp\").geometries())\n",
    "\n",
    "    minx, miny, maxx, maxy = imextent\n",
    "    lllon, lllat = minx, miny\n",
    "    urlon, urlat = maxx, maxy\n",
    "\n",
    "    # Create a Stamen Terrain instance.\n",
    "    stamen_terrain = cimgt.Stamen('terrain-background')\n",
    "\n",
    "    # Create a GeoAxes in the tile's projection.\n",
    "    gl = ax.gridlines(draw_labels=True)\n",
    "    gl.top_labels= False\n",
    "    gl.right_labels = False\n",
    "    if not latlabels:\n",
    "        gl.left_labels = False\n",
    "    if not lonlabels:\n",
    "        gl.bottom_labels = False\n",
    "    \n",
    "    gl.xlocator = mticker.FixedLocator([-122,-121,-120,-119])\n",
    "    gl.ylocator = mticker.FixedLocator(np.linspace(0,90,91))\n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER\n",
    "    gl.xlabel_style = {'size': 8, 'color': 'gray', 'rotation': 45}\n",
    "    gl.ylabel_style = {'size': 9, 'color': 'gray'}\n",
    "\n",
    "    # Limit the extent of the map to a small longitude/latitude range.\n",
    "    ax.set_extent([lllon, urlon, lllat, urlat])\n",
    "\n",
    "    # Add the Stamen data at zoom level 8.\n",
    "    ax.add_image(stamen_terrain, 8)\n",
    "    \n",
    "    im = ax.imshow(image,\n",
    "        cmap=cmap, zorder=1, vmin = 0, vmax = vmax,\n",
    "        origin=\"upper\", alpha = 0.8,\n",
    "        extent=(lllon, urlon, lllat, urlat),\n",
    "        transform=ccrs.PlateCarree(),\n",
    "    )\n",
    "\n",
    "    # Add shapefile outline\n",
    "    ax.add_feature(shape_feature, zorder = 2)\n",
    "\n",
    "    ax.scatter([point.x for point in points],\n",
    "               [point.y for point in points],\n",
    "               transform=ccrs.PlateCarree(), color= 'orange', s = 25, zorder = 10)\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(\"{}\".format(title), size = 12)\n",
    "    \n",
    "#     cbar = plt.colorbar(mappable=im,orientation='vertical', fraction=0.05, pad = 0.0125, ax = ax)\n",
    "\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d27e201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "PROCESSING : SJF\n",
      "************************************************************\n",
      "DONE ---\n",
      "************************************************************\n",
      "PROCESSING : TLG\n",
      "************************************************************\n",
      "DONE ---\n",
      "************************************************************\n",
      "PROCESSING : ISB\n",
      "************************************************************\n",
      "DONE ---\n",
      "************************************************************\n",
      "PROCESSING : NAT\n",
      "************************************************************\n",
      "DONE ---\n",
      "************************************************************\n",
      "PROCESSING : MHB\n",
      "************************************************************\n",
      "DONE ---\n",
      "************************************************************\n",
      "PROCESSING : NML\n",
      "************************************************************\n",
      "DONE ---\n",
      "************************************************************\n",
      "PROCESSING : ORO\n",
      "************************************************************\n",
      "DONE ---\n",
      "************************************************************\n",
      "PROCESSING : TRM\n",
      "************************************************************\n",
      "DONE ---\n",
      "************************************************************\n",
      "PROCESSING : MKM\n",
      "************************************************************\n",
      "DONE ---\n",
      "************************************************************\n",
      "PROCESSING : SCC\n",
      "************************************************************\n",
      "DONE ---\n",
      "************************************************************\n",
      "PROCESSING : PNF\n",
      "************************************************************\n",
      "DONE ---\n",
      "************************************************************\n",
      "PROCESSING : EXC\n",
      "************************************************************\n",
      "DONE ---\n",
      "************************************************************\n",
      "PROCESSING : NHG\n",
      "************************************************************\n",
      "DONE ---\n",
      "************************************************************\n",
      "PROCESSING : YRS\n",
      "************************************************************\n",
      "DONE ---\n",
      "************************************************************\n",
      "PROCESSING : SHA\n",
      "************************************************************\n",
      "DONE ---\n"
     ]
    }
   ],
   "source": [
    "# read files setup dirs \n",
    "gdf = gp.read_file(\"../shape/sierra_catchments.shp\")\n",
    "\n",
    "stids = list(gdf['stid'].values)\n",
    "\n",
    "# Define outdirs\n",
    "results_dir = \"../results/BF_src/\"\n",
    "outdir = os.path.join(results_dir,\"merged\")\n",
    "clipdir = os.path.join(results_dir,\"clipped\")\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n",
    "    \n",
    "if not os.path.exists(clipdir):\n",
    "    os.mkdir(clipdir)\n",
    "\n",
    "# Clip by watershed into the clipped dir\n",
    "for stn_id in stids[:]:\n",
    "    \n",
    "    print(\"****\" * 15)\n",
    "    print(\"PROCESSING : {}\".format(stn_id))\n",
    "    print(\"****\" * 15)\n",
    "\n",
    "    # get shapefile \n",
    "    catch_shp = os.path.abspath(\"../shape/{}.shp\".format(stn_id))\n",
    "    \n",
    "    # Get files matching the wshed \n",
    "    wshed_files = [os.path.join(results_dir,x) for x in os.listdir(results_dir) if stn_id in x]\n",
    "    wshed_files = [os.path.abspath(x) for x in wshed_files]\n",
    "    \n",
    "    for fn in wshed_files:\n",
    "        varfn = os.path.split(fn)[1]\n",
    "        outfn = os.path.join(clipdir,varfn) \n",
    "        if not os.path.exists(outfn):\n",
    "            cmd = '''gdalwarp -dstnodata -999 -cutline {} -crop_to_cutline {} {}'''.format(catch_shp, fn, outfn)\n",
    "            os.system(cmd)\n",
    "    print(\"DONE ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2372da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge files \n",
    "\n",
    "prcp_files = [os.path.join(clipdir,x) for x in os.listdir(clipdir) if \"prcp\" in x]\n",
    "\n",
    "# merge and write \n",
    "prcp_merged_fn = os.path.join(outdir,\"prcp.tiff\")\n",
    "if not os.path.exists(prcp_merged_fn):\n",
    "    g_prcp = gdal.Warp(prcp_merged_fn, prcp_files, format=\"GTiff\")\n",
    "    g_prcp = None # Close file\n",
    "    print(\"WROTE {}\".format(prcp_merged_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7a94de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "smlt_files = [os.path.join(clipdir,x) for x in os.listdir(clipdir) if \"smlt\" in x]\n",
    "\n",
    "smlt_merged_fn = os.path.join(outdir,\"smlt.tiff\")\n",
    "if not os.path.exists(smlt_merged_fn):\n",
    "    g_prcp = gdal.Warp(smlt_merged_fn, smlt_files, format=\"GTiff\")\n",
    "    g_prcp = None # Close file\n",
    "    print(\"WROTE {}\".format(smlt_merged_fn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89920daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the cmap \n",
    "\n",
    "cmap = get_cmap('cet_kbgyw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a496a82e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa618e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = rio.open(\"../results/BF_src/merged/smlt.tiff\")\n",
    "arr = src.read(1)\n",
    "arr[arr==src.nodata] = np.nan\n",
    "\n",
    "# SUMs\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1,figsize = (12,8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "im = plot_basemaps(arr, \"../shape/sierra_catchments.shp\", \"Snowmelt-Baseflow Correlation\", cmap = cmap, ax = axes, latlabels = True, lonlabels = True, vmax = 0.6)\n",
    "fig.subplots_adjust(wspace=0.02,hspace = 0.00)\n",
    "cax = fig.add_axes([axes.get_position().x1+0.01,axes.get_position().y0,0.02,axes.get_position().y1-axes.get_position().y0])\n",
    "fig.colorbar(mappable = im, orientation='vertical', cax = cax,ax = axes)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d117b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = rio.open(\"../results/BF_src/merged/prcp.tiff\")\n",
    "arr = src.read(1)\n",
    "arr[arr==src.nodata] = np.nan\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1,figsize = (12,8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "im = plot_basemaps(arr, \"../shape/sierra_catchments.shp\", \"Rainfall - Baseflow Correlation\", cmap = cmap, ax = axes, latlabels = True, lonlabels = True, vmax = 0.6)\n",
    "fig.subplots_adjust(wspace=0.02,hspace =0.00)\n",
    "cax = fig.add_axes([axes.get_position().x1+0.01,axes.get_position().y0,0.02,axes.get_position().y1-axes.get_position().y0])\n",
    "fig.colorbar(mappable = im, orientation='vertical', cax = cax,ax = axes)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELEVATION ANALYSIS \n",
    "\n",
    "# Read elevation contours we made \n",
    "cont_gdf = gp.read_file(\"../shape/contours_500m.shp\")\n",
    "cont_gdf['elev'] = np.linspace(250,4750,10) # [int(x) * 500 for x in gdf['ID']] \n",
    "\n",
    "# Set alpha val in cmap\n",
    "cmap = plt.get_cmap('viridis')\n",
    "cmap.set_under('k', alpha=0)\n",
    "\n",
    "# plot with watersehds \n",
    "ax = cont_gdf.plot(figsize = (12,8), column= 'elev',cmap = cmap, legend = True, vmin = 751)\n",
    "plt.title('500m elevation contours ')\n",
    "gp.read_file(\"../shape/sierra_catchments.shp\").plot(ax = ax, facecolor = \"none\", edgecolor = 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cdb9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 500m contour interval \n",
    "heights = np.linspace(0,5000,11)\n",
    "heights = [\"{} - {}\".format(str(x-500), str(x)) for x in heights]\n",
    "heights[0] = \"<0\"\n",
    "\n",
    "median_hts = np.linspace(250,4750,10)\n",
    "\n",
    "# Read each contour interval shape\n",
    "with fiona.open(\"../shape/contours_500m.shp\", \"r\") as shapefile:\n",
    "    shapes = [feature[\"geometry\"] for feature in shapefile]\n",
    "\n",
    "len(shapes), len(median_hts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674c9c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dir = \"../results/BF_src/merged/\"\n",
    "\n",
    "# Final results dict \n",
    "elev_res_fin = {}\n",
    "\n",
    "for vartype in ['smlt','prcp','ann_smlt','ann_prcp'][:]:\n",
    "    mean_fn = os.path.join(merged_dir,vartype+\".tiff\")\n",
    "    print(mean_fn)\n",
    "\n",
    "    # Set up dict for elevation results\n",
    "    elev_res = {}\n",
    "\n",
    "    # Loop through elevation contours\n",
    "    for idx, shape in enumerate(shapes):\n",
    "        with rasterio.open(mean_fn) as src:\n",
    "            out_image, out_transform = rasterio.mask.mask(src, [shape], crop=True)\n",
    "            outim = out_image.reshape(out_image.shape[1],out_image.shape[2])\n",
    "            outim[outim==src.nodata]=np.nan #mask nodata vals\n",
    "            outim[outim<=0] = np.nan\n",
    "            elev_res[heights[idx]] = outim[~np.isnan(outim)].flatten()\n",
    "\n",
    "    # Compile summary stats for each elevation bin \n",
    "    df_rows = []\n",
    "    for k,v in elev_res.items():\n",
    "        varmean = np.nanmean(v)\n",
    "        varstd = np.nanstd(v) #np.nanpercentile(v,66) - np.nanpercentile(v,50)\n",
    "        var_n = len(~np.isnan(v)) # count non - nan elements \n",
    "        sumdf = pd.DataFrame([k,varmean,varstd,var_n]).T\n",
    "        sumdf.columns = ['elev','{}_mean'.format(vartype),'{}_std'.format(vartype),'{}_num'.format(vartype)]\n",
    "        df_rows.append(sumdf)\n",
    "\n",
    "    # Concat the stats we just extracted \n",
    "    tdf = pd.concat(df_rows)\n",
    "    tdf['elev'] = np.linspace(250,4750,10)\n",
    "    tdf = tdf.astype(float).set_index(\"elev\")\n",
    "\n",
    "    elev_res_fin[vartype] = tdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c912b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get n pixels as f (elevation) by var type \n",
    "\n",
    "# Get number of obs for P\n",
    "sdf = elev_res_fin['prcp']\n",
    "pcols = [x for x in sdf.columns if \"prcp\" in x and \"num\" in x]\n",
    "psdf = pd.DataFrame(sdf[pcols].mean(axis = 1))\n",
    "psdf.columns =  [\"prcp\"]\n",
    "\n",
    "# Get number of obs for smlt\n",
    "sdf = elev_res_fin['smlt']\n",
    "scols = [x for x in sdf.columns if \"smlt\" in x and \"num\" in x]\n",
    "smdf = pd.DataFrame(sdf[scols].mean(axis = 1))\n",
    "smdf.columns = [\"smlt\"]\n",
    "\n",
    "num_df = pd.concat([psdf, smdf], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4938db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "masterdf = pd.concat(elev_res_fin.values(), axis = 1)\n",
    "masterdf = pd.concat(elev_res_fin.values(), axis = 1)\n",
    "masterdf.to_csv(\"../results/xc_elev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04356bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "# Means\n",
    "ax0 = plt.subplot(131)\n",
    "# ax0.set_title(\"Annual Means\", size = 14)\n",
    "\n",
    "ax0.errorbar(masterdf['ann_smlt_mean'],masterdf.index, xerr = masterdf['ann_smlt_std'], color = 'gray', label = 'snowmelt mean', linewidth = 2,marker='o')\n",
    "\n",
    "ax0.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax0.set_ylabel(\"elevation(m)\", size = 15)\n",
    "ax0.set_xlabel(\"Mean Annual Snowmelt (mm)\", size =14, color = 'gray')\n",
    "ax0.tick_params(axis='x', colors='gray')\n",
    "\n",
    "ax1 = ax0.twiny()  # instantiate a second axes that shares the same x-axis\n",
    "ax1.errorbar(masterdf['ann_prcp_mean'],masterdf.index, xerr = masterdf['ann_prcp_std'], color = 'blue', label = 'snowmelt mean', linewidth = 2,marker='o')\n",
    "\n",
    "ax1.set_xlabel(\"Mean Annual Precip (mm)\", size =14, color = 'blue')\n",
    "ax1.tick_params(axis='x', colors='blue')\n",
    "\n",
    "# Correlation plot \n",
    "ax2 = plt.subplot(132)\n",
    "# ax2.set_title(\"Annual Means\", size = 14)\n",
    "\n",
    "ax2.errorbar(masterdf['smlt_mean'],masterdf.index, xerr = masterdf['smlt_std'], color = 'gray', label = 'snowmelt mean', linewidth = 2,marker='o')\n",
    "\n",
    "ax2.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax2.set_ylabel(\"\")\n",
    "ax2.set_xlabel(\"Snowmelt - Baseflow Correlation\", size =14, color = 'gray')\n",
    "ax2.tick_params(axis='x', colors='gray')\n",
    "\n",
    "ax3 = ax2.twiny()  # instantiate a second axes that shares the same x-axis\n",
    "ax3.errorbar(masterdf['prcp_mean'],masterdf.index, xerr = masterdf['prcp_std'], color = 'blue', label = 'snowmelt mean', linewidth = 2,marker='o')\n",
    "\n",
    "ax3.set_xlabel(\"Precip - Baseflow Correlation\", size =14, color = 'blue')\n",
    "ax3.tick_params(axis='x', colors='blue')\n",
    "\n",
    "ax3.set(yticklabels=[])  \n",
    "ax3.set_ylabel(\"\")  \n",
    "\n",
    "plt.subplot(133)\n",
    "num_df.columns = ['Snowmelt', \"Precip\"]\n",
    "ax4 = num_df.plot(kind = 'barh', ax = plt.gca(),color = ['gray', 'blue'])\n",
    "ax4.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax4.set(yticklabels=[])  \n",
    "ax4.set_ylabel(\"\")  \n",
    "ax4.set_xlabel(\"Number of pixels\")  \n",
    "ax4.set_title(\"Number of pixels per area bin\", size = 14)\n",
    "ax4.set_xlim([0,31000])\n",
    "plt.show()\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d1961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760feb6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca65dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the same plots for every watershed\n",
    "\n",
    "cor_dir = \"../results/BF_src/\"\n",
    "mean_dir = \"../results/wshed_means/\"\n",
    "\n",
    "# Final results dict \n",
    "elev_res_sheds = {}\n",
    "\n",
    "for stid in tqdm(stids):\n",
    "    \n",
    "    stid_res = []\n",
    "    \n",
    "    for vartype in ['smlt','prcp'][:]:\n",
    "        fn = os.path.join(cor_dir,stid+\"_\"+vartype+\".tif\")\n",
    "        \n",
    "        elev_res = {}\n",
    "\n",
    "        # Loop through elevation contours\n",
    "        for idx, shape in enumerate(shapes):\n",
    "            try:\n",
    "                with rasterio.open(fn) as src:\n",
    "                    out_image, out_transform = rasterio.mask.mask(src, [shape], crop=True)\n",
    "                    outim = out_image.reshape(out_image.shape[1],out_image.shape[2])\n",
    "                    outim[outim==src.nodata]=np.nan #mask nodata vals\n",
    "                    outim[outim<=0] = np.nan\n",
    "                    elev_res[heights[idx]] = outim[~np.isnan(outim)].flatten()\n",
    "            \n",
    "            except:\n",
    "                dummy_arr = np.empty(10)\n",
    "                dummy_arr[:] = np.nan\n",
    "                elev_res[heights[idx]] = dummy_arr\n",
    "    \n",
    "        # Compile summary stats for each elevation bin \n",
    "        df_rows = []\n",
    "        for k,v in elev_res.items():\n",
    "            varmean = np.nanmedian(v)\n",
    "            varstd = np.nanstd(v) #np.nanpercentile(v,66) - np.nanpercentile(v,50)\n",
    "            var_n = len(~np.isnan(v)) # count non - nan elements \n",
    "            sumdf = pd.DataFrame([k,varmean,varstd,var_n]).T\n",
    "            sumdf.columns = ['elev','{}_{}_mean'.format(stid,vartype),'{}_{}_std'.format(stid,vartype),'{}_{}_num'.format(stid,vartype)]\n",
    "            df_rows.append(sumdf)\n",
    "\n",
    "        # Concat the stats we just extracted \n",
    "        tdf = pd.concat(df_rows)\n",
    "        tdf['elev'] = np.linspace(250,4750,10)\n",
    "        tdf = tdf.astype(float).set_index(\"elev\")\n",
    "        stid_res.append(tdf)\n",
    "        \n",
    "    elev_res_sheds[stid] = pd.concat(stid_res, axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26426555",
   "metadata": {},
   "outputs": [],
   "source": [
    "shed_df = pd.concat(elev_res_sheds.values(), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32fa5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scols = [x for x in shed_df.columns if \"smlt\" in x and \"num\" in x]\n",
    "sm_num_df = pd.DataFrame(shed_df[scols])\n",
    "sm_num_df.columns = [x[:3] for x in sm_num_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342b27e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(cc.glasbey, n_colors=len(stids))\n",
    "\n",
    "cmap_lookup = {}\n",
    "\n",
    "for idx,x in enumerate(stids):\n",
    "    cmap_lookup[x] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b73ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "# Means\n",
    "ax0 = plt.subplot(131)\n",
    "ax0.set_title(\"Snowmelt Correlation\", size = 14)\n",
    "\n",
    "for stid in stids:\n",
    "    ax0.errorbar(shed_df['{}_smlt_mean'.format(stid)],shed_df.index, xerr = shed_df['{}_smlt_std'.format(stid)], color = palette[cmap_lookup[stid]], linewidth = 1,marker='o', label = stid)\n",
    "\n",
    "ax0.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax0.set_ylabel(\"elevation(m)\", size = 15)\n",
    "ax0.set_xlabel(\"Snowmelt Correlation\", size =14)\n",
    "\n",
    "ax1 = plt.subplot(132)\n",
    "ax1.set_title(\"Rainfall Correlation\", size = 14)\n",
    "\n",
    "for stid in stids:\n",
    "    ax1.errorbar(shed_df['{}_prcp_mean'.format(stid)],shed_df.index, xerr = shed_df['{}_prcp_std'.format(stid)],  color = palette[cmap_lookup[stid]],linewidth = 1,marker='o', label = stid)\n",
    "\n",
    "# ax1.legend(loc = 'upper right', bbox_to_anchor = (1.25,1.05))\n",
    "ax1.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax1.set_ylabel(\"elevation(m)\", size = 15)\n",
    "ax1.set_xlabel(\"Rainfall Correlation\", size =14)\n",
    "ax1.set(yticklabels=[])  \n",
    "ax1.set_ylabel(\"\")  \n",
    "\n",
    "plt.subplot(133)\n",
    "ax4 = sm_num_df.plot(kind = 'barh',ax = plt.gca(),color = palette, width = 2)\n",
    "ax4.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax4.set(yticklabels=[])  \n",
    "ax4.set_ylabel(\"\")  \n",
    "ax4.set_xlabel(\"Number of pixels\", size = 12)  \n",
    "ax4.set_title(\"Number of pixels per area bin\", size = 14)\n",
    "ax4.legend(loc = 'upper right', fontsize = 12, bbox_to_anchor = (1.28,1.03))\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c3522c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01427d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot the mean annual value vs the correlation for each watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7522eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_dfs = []\n",
    "pr_dfs = []\n",
    "\n",
    "for stid in tqdm(stids[:]):\n",
    "    sm_mean_fn = os.path.join(\"../results/wshed_means/{}_smlt.tiff\".format(stid))\n",
    "    pr_mean_fn = os.path.join(\"../results/wshed_means/{}_prcp.tiff\".format(stid))\n",
    "    \n",
    "    sm_cor_fn = os.path.join(\"../results/BF_src/{}_smlt.tif\".format(stid))\n",
    "    pr_cor_fn = os.path.join(\"../results/BF_src/{}_prcp.tif\".format(stid))\n",
    "    \n",
    "    sm_t = rio.open(sm_mean_fn)\n",
    "    sm_mean = sm_t.read(1)\n",
    "    sm_mean[sm_mean==sm_t.nodata] = np.nan\n",
    "    \n",
    "    pr_t = rio.open(pr_mean_fn)\n",
    "    pr_mean = sm_t.read(1)\n",
    "    pr_mean[pr_mean==pr_t.nodata] = np.nan\n",
    "    \n",
    "    pr_c_t = rio.open(pr_cor_fn)\n",
    "    pr_cor = pr_c_t.read(1)\n",
    "    pr_cor[pr_cor==pr_c_t.nodata] = np.nan\n",
    "    pr_cor[pr_cor<0] = np.nan\n",
    "    \n",
    "    sm_c_t = rio.open(sm_cor_fn)\n",
    "    sm_cor = sm_c_t.read(1)\n",
    "    sm_cor[sm_cor==sm_c_t.nodata] = np.nan\n",
    "    sm_cor[sm_cor<0] = np.nan\n",
    "\n",
    "    smdf = pd.DataFrame([sm_mean.flatten(),sm_cor.flatten()]).T\n",
    "    smdf.columns = ['sm_mean','sm_cor']\n",
    "    smdf['id'] = [stid for x in range(0,len(smdf))]\n",
    "    sm_dfs.append(smdf)\n",
    "    \n",
    "    prdf = pd.DataFrame([pr_mean.flatten(),pr_cor.flatten()]).T\n",
    "    prdf.columns = ['pr_mean','pr_cor']\n",
    "    prdf['id'] = [stid for x in range(0,len(prdf))]\n",
    "    pr_dfs.append(prdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee86a6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "smdf_fin = pd.concat(sm_dfs, axis = 0).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67cc9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot('sm_mean', 'sm_cor', data=smdf_fin, hue='id', palette = palette, scatter_kws={'alpha':0.9})\n",
    "plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a361cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prdf_fin = pd.concat(pr_dfs, axis = 0).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eca25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot('pr_mean', 'pr_cor', data=prdf_fin, hue='id', palette = palette, scatter_kws={'alpha':0.9})\n",
    "plt.ylim([0,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378f6877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
